## 数据降维

下面是几种常见的降维算法的介绍、优缺点的表格：

| 名称                       | 介绍                                                         | 优缺点                                                       |
| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 主成分分析（PCA）          | 主成分分析是一种常用的**线性降维**方法，通过找到数据中方差最大的投影方向，将数据映射到较低维度的子空间。 | 优点：简单易实现；能够保留原始数据中的大部分方差；适用于线性相关的数据。<br>缺点：对非线性关系的数据效果不佳；无法处理非高斯分布的数据。 |
| 线性判别分析（LDA）        | 线性判别分析是一种监督降维方法，旨在找到能够最大程度地区分不同类别的投影方向。它能够在降维的同时保留类别间的区分性。 | 优点：能够提高分类性能；能够保留类别信息；适用于有监督学习任务。<br>缺点：对于非线性问题效果较差；在类别不平衡或维度较高时可能不稳定。 |
| t分布随机邻近嵌入（t-SNE） | t-SNE是一种非线性降维方法，用于可视化高维数据。它通过在高维空间中保持样本间的相对距离，并将其映射到较低维度的空间中，以揭示数据的聚类和结构。 | 优点：适用于可视化高维数据；能够保留样本间的相对距离；在发现聚类和结构方面表现良好。<br>缺点：计算复杂度高；结果依赖于随机初始化；不适用于保留全局结构。 |
| 非负矩阵分解（NMF）        | 非负矩阵分解是一种非线性降维方法，适用于处理非负数据。它将原始数据矩阵表示为两个非负矩阵的乘积，以提取潜在的特征表示。 | 优点：适用于处理非负数据；对于非线性问题效果较好；能够提取潜在特征。<br>缺点：结果依赖于初始值；计算复杂度较高；无法保留全局结构。 |
| 独立成分分析（ICA）        | 独立成分分析是一种用于盲源分离和降维的方法，假设观测数据是若干个相互独立的信号的线性组合。ICA试图找到这些独立信号。 | 优点：适用于盲源分离任务；能够恢复潜在独立信号；对于非高斯分布的数据效果较好。<br>缺点：结果依赖于数据的独立性假设；对于高度相关的信号效果较差；对噪声敏感。 |

请注意，这些算法的适用性和性能取决于数据的特点和具体任务的要求。因此，在选择降维算法时，需要根据具体情况进行评估和选择。

## PCA 主成分分析及其可视化

主成分分析（Principal Component Analysis，PCA）是一种常用的数据降维技术。它可以将高维数据转换为低维数据，同时保留尽可能多的原始数据信息。

在正式介绍主成分分析之前，需要先了解一些相关概念：

1. 方差：描述数据分布的离散程度，方差越大表示数据越分散。

2. 协方差：描述两个变量之间的关系，协方差值为正表示两个变量同向变化，协方差值为负表示两个变量反向变化，协方差值接近于0表示两个变量基本独立。

3. 特征向量：对于一个矩阵A，如果存在一个非零向量v，使得Av=λv，其中λ是一个常数，那么v就是A的特征向量，λ就是v对应的特征值。

有了上面的几个概念，就可以开始介绍主成分分析了。

主成分分析的目标是找到数据中最重要的主成分，也就是能够解释数据变化最大的方向。这些主成分是通过对数据的协方差矩阵进行特征值分解得到的。协方差矩阵是一个对称矩阵，对角线上的元素表示每个变量的方差，非对角线上的元素表示两个变量之间的协方差。特征值分解可以将协方差矩阵分解为特征向量和特征值的乘积形式。

具体地，设有m个n维数据样本，将它们组成一个$m\times n$的矩阵X。则X的协方差矩阵为：$$C=\frac{1}{m-1}X^TX$$ 对$C$进行特征值分解，得到特征向量$v_1,v_2,\dots,v_n$和相应的特征值$\lambda_1,\lambda_2,\dots,\lambda_n$。这些特征向量构成了原始数据空间的一组新基，其中第$i$个特征向量$v_i$对应的特征值$\lambda_i$表示数据在这个方向上的方差大小。

按照特征值的大小排序，选取前$k$个特征向量$v_1,v_2,\dots,v_k$，将原始数据投影到这$k$个特征向量张成的子空间中，得到降维后的数据$Y=XV_k$，其中$V_k$是由前$k$个特征向量构成的$n\times k$矩阵，$Y$是$m\times k$矩阵，即将每个样本从$n$维压缩到$k$维。

这里有一个重要的性质：通过PCA降维得到的新数据，每个特征之间的协方差为0。这意味着降维后的数据之间相互独立，在某些情况下可以简化后续的计算过程。

**需要注意的是，PCA只能用于线性数据降维，对于非线性数据降维，需要使用其他技术，比如核主成分分析（Kernel PCA）等。**

总结一下PCA的步骤：

1. 对原始数据进行标准化处理，使每个变量的均值为0，方差为1。

2. 计算协方差矩阵，并进行特征值分解。

3. 按照特征值的大小排序，选取前$k$个特征向量构成新的基，将原始数据投影到这$k$个特征向量张成的子空间中，得到降维后的数据。


      4. 可选：对降维后的数据进行可视化分析，以便更好地理解数据集的结构和关系。
    
      5. 使用降维后的数据进行后续分析，比如聚类、分类等。

   需要注意的是，PCA在实际应用中也有一些限制和注意事项：

      1. 降维后的数据往往难以解释，因为它们是由多个原始特征线性组合而成的。
    
      2. PCA只能减少特征数量，无法增加新的特征。如果样本数量较少，或者某些特征之间存在非线性关系，使用PCA可能会导致信息丢失。
    
      3. 在计算协方差矩阵时，如果变量之间存在相关性，会导致协方差矩阵的条件数较大，从而使得特征值分解的计算量变得很大。
    
      4. 当数据集中存在离群点或异常值时，PCA对降维结果会产生较大影响，此时需要进行数据清洗或采用其他方法来处理异常值。

   总之，PCA是一种常用的数据降维技术，可以有效地解决高维数据分析问题。但是，在实际应用中，需要根据具体情况选择合适的降维方法，并进行必要的预处理和后处理操作，以获得更好的分析结果。

### t-SNE

### PCA



### 数据标准化和归一化

在使用梯度下降算法进行模型训练时，对输入特征进行比例缩放（或归一化）有以下几个原因：

1. 加速收敛：梯度下降的目标是找到损失函数最小化的参数值，而不同特征可能具有不同的尺度和范围。如果某些特征具有较大的值范围，那么**其相关权重更新也会更大，这可能导致算法收敛过程变得非常缓慢甚至无法收敛。**通过对输入特征进行比例缩放，可以使各个特征都处于相似的尺度范围内，从而加快算法收敛速度。

2. 防止数值溢出：在计算过程中，**涉及到较大或较小数值时容易发生数值溢出问题。**通过将输入特征进行比例缩放，可以有效地避免这种情况的发生。

3. 提高模型性能：某些机器学习模型（如支持向量机、K近邻等）对输入数据中不同尺度和范围非常敏感。**当存在明显差异的尺度时，在距离计算、权重分配等方面可能会产生偏差，并且影响模型性能**。通过比例缩放输入特征，可以确保模型能够更好地利用每个特征的信息，提高模型性能。

在线性回归中，尤其是多变量回归模型，由于各个的数据之间量化纲位不同，如果说两个参数尺度范围分别是是【0~1000，0 ~5】或者【-0.00004 ~ 0.00002，10 ~ 30】,  那么在使用梯度下降算法时，他们的等高线是一个又窄又高的等高线，如下图：   

![在这里插入图片描述](data preprocessing.assets/ed4bf86589724f8eb5b61e1fdbee9ccb.png)


 因为一个他们量化纲位不同会出现 （1，299），（3，800） 这种特征实例，那么等高线就会又窄又高，在梯度下降算法中，参数更新就会如上图左右震荡（权重更新一点就会导致输出变大，对大尺度的特征更加敏感，不利于学习）如果等高线如下图，参数更新就能更快收敛与更新了

![在这里插入图片描述](data preprocessing.assets/17b724491d0446e79d13f0d763ce9eab.png)

如下图：代价函数（如MSE: 回归模型的预测值和实际值的差的平方和）的3D图，就像下山一样，为了达到局部最优点或全局最优点，作为下山者，你肯定希望地形比较平缓，比较清楚的知道往哪里走能够最快下山，而如果这个山又陡又窄，那下山者是不是下山肯定速度慢很多（更新中不能较快收敛，左右震荡），往哪里都是下降，不能准确找到方向。

**理想的代价函数**

![在这里插入图片描述](data preprocessing.assets/d0fe871ee702492c95dc9935a144d462.png)

**但实际往往都是下图的情况** （有许多局部最优）

![在这里插入图片描述](data preprocessing.assets/998fd4c4f7fa42fa92569882d19ab44e.png)

数据标准化和归一化是常见的数据预处理技术，它们在以下情况下使用：

1. 特征缩放：当特征的取值范围差异较大时，可以使用数据标准化或归一化来将其缩放到相似的范围。这有助于**避免某些特征对模型训练产生过大影响**。

2. 收敛加速：在某些机器学习算法（如梯度下降）中，如果不进行数据标准化或归一化，则可能需要更多迭代次数才能收敛到最优解。通过使特征**具有类似的尺度，可以提高算法收敛速度**（不再左右震荡，权重更新性价比相同，特征之前模型一视同仁）并加快训练过程。

3. 防止数值溢出：当输入数据包含非常大或非常小的值时，计算中可能会发生**数值溢出或舍入误差**。通过将数据缩放到合理范围内，可以避免这些问题，并提高计算稳定性。

4. 算法要求：某些机器学习算法（如K均值聚类、支持向量机等）对输入数据进行了假设，**例如假设样本服从正态分布。**（机器学习算法最重要的概率统计，如果尺度和范围不同，显然很难拟合分布）在这种情况下，对于满足这些假设的算法而言，数据标准化或归一化是必要的预处理步骤。

尽管数据标准化和归一化在许多情况下都很有用，但并不是所有算法都需要进行这些操作。例如，**决策树和随机森林等基于树的模型通常不受特征缩放影响**（这种是基于不同类别的信息增益（信息熵）或者基尼指数（类别纯度）确定阈值，而图像识别等深度学习任务则通常对原始输入进行归一化处理。

在实践中，可以使用以下方法来进行数据标准化和归一化：

- 数据标准化（Standardization）：通过将每个特征值减去其均值，并除以其标准差来使特征**具有零均值和单位方差。**（这也是我们在平时数学上求正态分布的算法，使其均值为0，方差为1）

- 数据归一化（Normalization）：通过将每个特征值按比例缩放到给定范围内（如0到1之间）来**保持相对关系**（比如最大最小值归一化，或者同除于最大值)。

   >  数据归一化是将不同尺度的数据转换为统一尺度范围的过程。常见的数据归一化算法包括：
   >
   >  1. 最小-最大缩放（Min-Max Scaling）：该方法通过**线性变换**将原始数据映射到[0, 1]或者[-1, 1]之间。公式如下：
   >     $$
   >     X_{\text{new}} = \frac{{X - X_{\text{min}}}}{{X_{\text{max}} - X_{\text{min}}}}
   >     $$
   >     其中，\(X\) 是原始输入特征，\(X_{\text{new}}\) 是进行归一化后得到的新特征。以下代码为实现（可以缩放到制定范围）
   >
   >     ```python
   >     X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) # 每个值缩放到0-1
   >     X_scaled = X_std * (max - min) + min  # 中间值缩放后的结果
   >     ```
   >     
   >  3. 小数定标缩放（Decimal Scaling）：该方法通过**移动小数点位置**来实现归一化处理。具体做法是**找出使得所有样本都落在[-1, 1]之间的最小整数n，然后将原始数据除以$10^n$**。公式如下：
   >     $$
   >     X_{\text{new}} = \frac{{X}}{{10^n}}
   >     $$
   >      其中，$X$ 是原始输入特征，$X_{\text{new}}$ 是进行归一化后得到的新特征。
   >
   >  >  1. **不受异常值影响**：当存在异常值时，使用最大-最小缩放可能会导致归一化后的数据范围过于集中或扩散。而通过移动小数点位置来进行归一化，则可以**避免受到极端值的干扰**。
   >  >
   >  >  2. **保留原始数据分布形态**：移动小数点位置不改变原始数据之间的相对关系，只是将其映射到[-1, 1]区间内。这种方式更加**保留了原始数据分布形态**，有助于保持特征之间的相对差异性。
   >  >
   >  >  3. 方便计算和理解：与直接使用公式进行线性变换相比，寻找使得所有样本都落在[-1, 1]之间的最小整数n方法**更容易理解和实现**。它不需要具体计算每个特征的最大和最小值，并且**仅涉及整数运算**。
   >  >
   >  >  然而，在某些情况下，使用该方法也存在一些限制：
   >  >
   >  >  - **如果你希望将数据映射到其他范围而非[-1, 1]，此方法就无法满足需求**。
   >  >- **在处理稀疏数据时，该方法可能会导致数据密度不均匀**。

请注意，在	应用这些技术时，应该先分割出训练集和测试集，并且**仅使用训练集上的统计信息来**对整个数据集进行转换。然后将相同的变换应用于测试集以确保结果的可靠性。

在选择数据标准化（Standardization）和数据归一化（Normalization）之前，需要根据具体情况来评估它们的优劣。

1. 数据标准化的优点：
   - **保留了原始数据分布的形状，不会改变特征的相对关系**。
   - **对异常值不敏感**。由于使用均值和标准差进行缩放，**异常值对结果影响较小**。
   - 在某些机器学习算法中表现较好，如逻辑回归、线性回归等基于距离计算或梯度下降的模型。

2. 数据归一化的优点：
   - 将特征**缩放到固定范围**内有**利于比较不同单位或取值范围的特征**。
   - 有助于加速收敛过程，在某些机器学习算法中可能提高训练速度。
   - 在某些算法要求输入数据处于特定范围时非常有用，如支持向量机、K均值聚类等。

因此，在选择数据标准化还是数据归一化时可以考虑以下因素：

- 数据分布：观察数据的分布情况。如果**数据分布偏态较大**，或者包含**明显的离群值**（异常值），则数据标准化可能更合适（将数据放正）。因为数据标准化使用的是均值和标准差，对异常值相对不敏感。而数据归一化可能会受到异常值的影响，因为它将数据缩放到0到1的范围。（将数据缩放变小）
- 数据要求：考虑数据在具体应用中的要求。如果应用场景对数据的绝对值大小没有特别要求，而**更关注数据的相对关系和比例**，那么数据归一化可能更适合。例如，某些机器学习算法（如支持向量机和K近邻算法）对数据的尺度敏感，此时进行数据归一化可以确保数据在相同的尺度上进行比较。
- 数据特征：考虑数据的特征和取值范围。**如果数据具有异质性，即不同特征具有不同的单位和尺度，数据标准化可以消除特征之间的差异**。而如果数据特征之间的差异不大，或者**数据已经处于相对相似的尺度上，数据归一化可能更简单和适用。**

需要注意的是，数据归一化和数据标准化只是常见的数据预处理方法之一，具体的选择还应考虑特定问题的要求和数据的特性。在实践中，可以进行实验和比较不同方法的效果，以确定最适合的数据预处理方式。另外，还可以考虑其他预处理技术，如对数转换、特征缩放等，以满足具体问题的需求。（实践是证明真理的唯一标准！！！ —— 伽利略） 

> 如下图所示，这是一个自动售货机每日销量总值预测数据标准化和最大最小值归一化的图例，可以看到三种趋势都没有变化，而只是数值范围发生变化，这就很好的去除了量纲不统一的问题，除此之外，**在这里的数据如果拿去跑神经网络，最大最小值归一化效果会更好（0-1），尤其是小数据量，更容易学习，由于其原始数据没有偏态较大，和特别的异常值所以可以选取最大最小值归一化**

<img src="data%20processing.assets/image-20231224150249848.png" alt="image-20231224150249848" style="zoom:50%;" />

### 离散化与连续化

离散化（Discretization）和连续化（Continuousization）是数据处理中的两个相对概念，用于处理不同类型的数据。

离散化是将连续型数据或特征转换为离散型的过程。它将连续的取值范围划分为有限的离散区间或类别。离散化的目的是**简化数据表示**、**降低数据噪声影响**、改善模型性能等。常见的离散化方法包括等宽离散化（Equal Width Discretization）、等频离散化（Equal Frequency Discretization）和基于聚类的离散化（Clustering-based Discretization）等。

连续化是将离散型数据或特征转换为连续型的过程。它将离散的类别或取值转换为连续的数值表示。连续化的目的是将离散的特征转换为可用于**连续型数据分析和建模**的形式，以便于应用各种连续型数据处理技术。常见的连续化方法包括独热编码（One-Hot Encoding）、特征哈希（Feature Hashing）和嵌入（Embedding）等。

离散化和连续化在数据处理中有不同的应用场景和目的：

1. 离散化适用于处理连续型数据，将其转换为离散型数据。离散化常用于特征工程、数据预处理和某些机器学习算法中。例如，将年龄分为不同的年龄段或将收入分为不同的收入水平类别。

2. 连续化适用于处理离散型数据，将其转换为连续型数据。连续化常用于将分类变量转换为可用于连续型数据建模的形式。例如，在自然语言处理任务中，将**文本的词汇或类别转换为连续的词嵌入向量**。

需要注意的是，在离散化和连续化过程中，数据的特性和任务的要求应该被充分考虑。离散化和连续化的选择应**基于数据的分布**、**特征的含义**和具体任务的需要。此外，离散化和连续化可能会引入**信息损失**或导致**数据的变形**，因此在应用中需要谨慎处理。

>  pd.cut() 可以将数据按照区间(可对应标签) 离散化

